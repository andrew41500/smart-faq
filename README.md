## Smart Multi-Agent FAQ & Knowledge Assistant

A full-stack demo project showcasing **LangGraph multi-agent orchestration**, **RAG with ChromaDB**, **Flask**, **Docker**, and **CI/CD to Railway**.

### Architecture Overview

- **Frontend**: `index.html` + vanilla JS/CSS chat UI.
- **Backend**: Flask API in `backend/app.py`.
- **Workflow Orchestration**: LangGraph graph in `backend/graph/workflow.py` with nodes in `backend/graph/nodes.py`.
- **LLM Abstraction**: `backend/llm.py` (single model, different prompts per node).
- **RAG & Vector Store**: ChromaDB with embeddings via `sentence-transformers` in `backend/rag/*`.
- **Sample Docs**: Generated by `backend/sample_docs_generator.py`.
- **CI/CD**: GitHub Actions workflow `.github/workflows/ci.yml`.
- **Containerization**: `Dockerfile` and `docker-compose.yml`.

#### LangGraph Workflow

```text
[User Query]
      |
      v
Intent Classification (GENERAL vs DOCUMENT)
      |
      +------------------------------+
      | DOCUMENT                     | GENERAL
      v                              v
Document Retrieval (ChromaDB)   General QA Agent
      |                              |
      +--------------+---------------+
                     v
           Answer Generation (doc-based)
                     |
                     v
             Summarization (optional)
                     |
                     v
           Fallback (human escalation)
                     |
                     v
               Final Answer
```

### Folder Structure (key parts)

```text
.
├─ backend/
│  ├─ app.py                 # Flask app + /api/chat endpoint
│  ├─ llm.py                 # LLM provider abstraction (Google Gemini or HF)
│  ├─ sample_docs_generator.py
│  ├─ graph/
│  │  ├─ workflow.py         # LangGraph graph definition
│  │  └─ nodes.py            # Intent, retrieval, QA, summarization, fallback
│  └─ rag/
│     ├─ parsers.py          # PDF/DOCX parsing
│     ├─ retriever.py        # ChromaDB vector store + retriever
│     └─ ingest.py           # Ingestion script for sample docs
├─ frontend/
│  ├─ templates/index.html   # Chat UI
│  └─ static/
│     ├─ css/style.css
│     └─ js/app.js
├─ tests/
│  └─ test_smoke.py          # Simple CI smoke test
├─ Dockerfile
├─ docker-compose.yml
├─ requirements.txt
└─ .github/workflows/ci.yml
```

### Local Setup & Run

1. **Clone & install dependencies**

```bash
git clone <your-repo-url> smart-faq
cd smart-faq
python -m venv .venv
. .venv/Scripts/activate  # Windows PowerShell: .venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

2. **Set environment variables**

Choose a provider (recommended: Google Gemini):

```bash
set LLM_PROVIDER=google
set GOOGLE_API_KEY=your-google-api-key
```

Or Hugging Face:

```bash
set LLM_PROVIDER=huggingface
set HUGGINGFACE_API_TOKEN=your-hf-token
```

3. **Generate sample documents and ingest into Chroma**

```bash
python -m backend.sample_docs_generator
python -m backend.rag.ingest
```

4. **Run the Flask app locally**

```bash
set FLASK_APP=backend.app
python backend/app.py
```

Open `http://localhost:8000` and start chatting.

### Sample Queries

- **General questions**
  - “What is SmartSupport Cloud and who is it for?”
  - “How does SmartSupport Cloud help my support team?”
- **Document-based questions**
  - “Explain the onboarding steps from the manual.”
  - “How is my data stored and secured?”
  - “Does SmartSupport Cloud support multi-agent workflows?”
- **Fallback trigger**
  - “What is your refund policy for on-premise appliances?”

These are designed to demonstrate:
- Intent routing (GENERAL vs DOCUMENT).
- Document retrieval from the generated PDF/DOCX.
- Summarization when answers are long.
- Fallback message when manuals don’t contain the answer.

### Docker Usage

Build and run with Docker:

```bash
docker build -t smart-faq .
docker run -p 8000:8000 ^
  -e LLM_PROVIDER=google ^
  -e GOOGLE_API_KEY=your-google-api-key ^
  -v %cd%/chroma_data:/data/chroma_db ^
  smart-faq
```

Or with `docker-compose` (recommended for local dev):

```bash
docker-compose up --build
```

Before first run inside the container, you can exec into it to generate & ingest docs:

```bash
docker-compose run --rm app bash -lc "python -m backend.sample_docs_generator && python -m backend.rag.ingest"
```

### Railway Deployment (Free Tier)

1. **Create a new Railway project**
   - Sign in to Railway.
   - Create a new project and select “Deploy from GitHub”.
   - Connect your GitHub repo that contains this project.

2. **Docker deployment**
   - Railway auto-detects the `Dockerfile`.
   - It will build and run using `CMD ["gunicorn", "-b", "0.0.0.0:8000", "backend.app:create_app()"]`.

3. **Configure environment variables in Railway**

Set (in the Railway dashboard under Variables):

- `LLM_PROVIDER=google` (or `huggingface`)
- `GOOGLE_API_KEY` or `HUGGINGFACE_API_TOKEN`
- `CHROMA_DIR=/data/chroma_db`
- `SAMPLE_DOCS_DIR=/app/sample_docs`

4. **Configure persistent volume for Chroma**

In the Railway service:
- Add a new **volume**, e.g. mount path `/data`.
- Ensure `CHROMA_DIR` is inside this volume (e.g. `/data/chroma_db`).

5. **Ingest sample docs on Railway**

You can run a one-off Railway shell/CLI command:

```bash
railway run "python -m backend.sample_docs_generator && python -m backend.rag.ingest"
```

This will populate Chroma in your mounted volume, so subsequent deployments retain the vector store.

6. **CI/CD with GitHub Actions**

- The workflow in `.github/workflows/ci.yml`:
  - Installs dependencies.
  - Runs `flake8` on the `backend` package.
  - Runs `pytest`.
  - On pushes to `main`, calls `railway up` to deploy.
- In GitHub repo **Settings → Secrets and variables → Actions**:
  - Add `RAILWAY_TOKEN` (generated from Railway account).

Push to `main` to trigger lint, tests, and deployment automatically.

### 2-Minute Live Demo Script

You can use this during an interview:

1. **Introduce the app (20–30s)**
   - “This is a Smart Multi-Agent FAQ & Knowledge Assistant. It uses LangGraph to orchestrate multiple agents — intent classification, document retrieval, answer generation, summarization, and a human escalation fallback — all backed by a single LLM.”

2. **Show architecture (30s)**
   - Briefly point to the LangGraph in `backend/graph/workflow.py` and the ASCII diagram in the README.
   - Explain: “The user query first hits the intent classifier. If it’s document-heavy, we retrieve chunks from a ChromaDB built from PDFs/DOCX; otherwise we use a general FAQ prompt. Both paths flow into a summarizer node, then a fallback node if we still don’t have a confident answer.”

3. **Run through example queries (40–60s)**
   - Ask a **general question**: “What is SmartSupport Cloud and who is it for?” → show general FAQ answer and intent label GENERAL.
   - Ask a **document-based question**: “Explain the onboarding steps from the manual.” → show that the answer is grounded in manual text and the intent is DOCUMENT with source filenames.
   - Ask a **no-answer question**: “What is your refund policy for on-premise appliances?” → demonstrate the fallback message “I’m escalating this to a human.”
   - Toggle **Summarize long answers** and show a more concise response.

4. **Highlight production-readiness (20–30s)**
   - Mention Dockerfile, Railway deployment with a persistent Chroma volume, and CI pipeline that runs lint/tests and can auto-deploy via the Railway CLI.
   - Emphasize how new agents (e.g., translation, sentiment) can be added as additional LangGraph nodes wired into the same `AssistantState`.

### Deployment URL

Once deployed on Railway, add your public URL here for recruiters:

- **Demo URL**: `https://your-railway-app-url.up.railway.app`


