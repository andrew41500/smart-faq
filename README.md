## Smart Multi-Agent FAQ & Knowledge Assistant

A full-stack demo project showcasing **LangGraph multi-agent orchestration**, **RAG with ChromaDB**, **Flask**, **Docker**, and **CI/CD to Railway**.

### Architecture Overview

- **Frontend**: `index.html` + vanilla JS/CSS chat UI.
- **Backend**: Flask API in `backend/app.py`.
- **Workflow Orchestration**: LangGraph graph in `backend/graph/workflow.py` with nodes in `backend/graph/nodes.py`.
- **LLM Abstraction**: `backend/llm.py` (single model, different prompts per node).
- **RAG & Vector Store**: ChromaDB with embeddings via `sentence-transformers` in `backend/rag/*`.
- **Sample Docs**: Generated by `backend/sample_docs_generator.py`.
- **CI/CD**: GitHub Actions workflow `.github/workflows/ci.yml`.
- **Containerization**: `Dockerfile` and `docker-compose.yml`.

#### LangGraph Workflow

```text
[User Query]
      |
      v
Intent Classification (GENERAL vs DOCUMENT)
      |
      +------------------------------+
      | DOCUMENT                     | GENERAL
      v                              v
Document Retrieval (ChromaDB)   General QA Agent
      |                              |
      +--------------+---------------+
                     v
           Answer Generation (doc-based)
                     |
                     v
             Summarization (optional)
                     |
                     v
           Fallback (human escalation)
                     |
                     v
               Final Answer
```

### Folder Structure (key parts)

```text
.
├─ backend/
│  ├─ app.py                 # Flask app + /api/chat endpoint
│  ├─ llm.py                 # LLM provider abstraction (Google Gemini or HF)
│  ├─ sample_docs_generator.py
│  ├─ graph/
│  │  ├─ workflow.py         # LangGraph graph definition
│  │  └─ nodes.py            # Intent, retrieval, QA, summarization, fallback
│  └─ rag/
│     ├─ parsers.py          # PDF/DOCX parsing
│     ├─ retriever.py        # ChromaDB vector store + retriever
│     └─ ingest.py           # Ingestion script for sample docs
├─ frontend/
│  ├─ templates/index.html   # Chat UI
│  └─ static/
│     ├─ css/style.css
│     └─ js/app.js
├─ tests/
│  └─ test_smoke.py          # Simple CI smoke test
├─ Dockerfile
├─ docker-compose.yml
├─ requirements.txt
└─ .github/workflows/ci.yml
```

### Local Setup & Run

1. **Clone & install dependencies**

```bash
git clone <your-repo-url> smart-faq
cd smart-faq
python -m venv .venv
. .venv/Scripts/activate  # Windows PowerShell: .venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

2. **Set environment variables**

Choose a provider (recommended: Google Gemini):

```bash
set LLM_PROVIDER=google
set GOOGLE_API_KEY=your-google-api-key
```

Or Hugging Face:

```bash
set LLM_PROVIDER=huggingface
set HUGGINGFACE_API_TOKEN=your-hf-token
```

3. **Generate sample documents and ingest into Chroma**

```bash
python -m backend.sample_docs_generator
python -m backend.rag.ingest
```

4. **Run the Flask app locally**

```bash
set FLASK_APP=backend.app
python backend/app.py
```

Open `http://localhost:8000` and start chatting.

### Sample Queries

- **General questions**
  - “What is SmartSupport Cloud and who is it for?”
  - “How does SmartSupport Cloud help my support team?”
- **Document-based questions**
  - “Explain the onboarding steps from the manual.”
  - “How is my data stored and secured?”
  - “Does SmartSupport Cloud support multi-agent workflows?”
- **Fallback trigger**
  - “What is your refund policy for on-premise appliances?”

These are designed to demonstrate:
- Intent routing (GENERAL vs DOCUMENT).
- Document retrieval from the generated PDF/DOCX.
- Summarization when answers are long.
- Fallback message when manuals don’t contain the answer.

### Docker Usage

Build and run with Docker:

```bash
docker build -t smart-faq .
docker run -p 8000:8000 ^
  -e LLM_PROVIDER=google ^
  -e GOOGLE_API_KEY=your-google-api-key ^
  -v %cd%/chroma_data:/data/chroma_db ^
  smart-faq
```

Or with `docker-compose` (recommended for local dev):

```bash
docker-compose up --build
```

Before first run inside the container, you can exec into it to generate & ingest docs:

```bash
docker-compose run --rm app bash -lc "python -m backend.sample_docs_generator && python -m backend.rag.ingest"
```




